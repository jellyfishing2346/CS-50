   1) Created a layer with 32 filters and a 3x3 kernel, one max-pooling layer with 2x2 pool size, one hidden layer with 128 units and a 0.5 dropout. Also, only one hidden was certainly insufficient for such a complex data set.
    2)I then experimented by adding two more hidden layers with 128 units and the same relu activation. The information was better with the use of more layers made the model train itself better on the variety of signs in the data set.
    3)  With the idea of avoiding over-fitting in mind, I decided to add more dropout layers. I ended up adding two more dropout layers after two last hidden layers. The probability I set was 0.05, for both actually seemed to worsen the situation. The issue is that many dropouts did not let the model train work properly.
    4) Finally, I decided to have only one dropout of 0.5 probability and add another convolutional layer to generalize the images better.  The extra layer probably helped by making the images look more like other images, which is useful for the model.
    5) Actually, it seems like having another pooling layer after the second convolutional layer helps to lower the loss while preserving the accuracy.